Hey everyone, my name is Greg Hogg and welcome to my channel. Today we'll be forecasting Microsoft stock using LSTM neural networks. This is a very important project to put on your resume, so I'd really highly recommend watching the video in its entirety. I made it as clear and concise as I possibly could, so I really think you're going to find this useful. Enjoy the video, I'll see you in there.
We first want to grab the dataset which we can get from this Yahoo Finance link here, which will bring us to Microsoft Corporation stock page. We can scroll down and change the time period from one year to max to get all of the information and then click apply. We want to download, which will bring a csv to your computer. We need to bring that csv into our environment, so in Google Collab we go here and then upload the file. We can simply rename it by deleting those extra characters and pressing enter. So we will do import pandas as pd and make df equal to dot read csv passing the file name of msft.csv, close that and then outputting df. 9082 rows of stock information, it goes all the way from the beginning (1986) all the way till now (today which is 2022 March 23rd). If you're following along you might see a different date here, closer to your today. Notice that we don't trade stocks every single day, there's a gap here (19 and 20 don't exist) and many other pieces in the middle don't exist as well. That is okay.
Looking at the different columns of the data set, we have the date and on that date what the stock opened at, the highest value for that day, the lowest value for that day, what it closed at, the adjusted closing value, and then the volume of stocks traded that day. We're going to keep things simple by just using the closing value, so we'll have the date and what that value was at the end of that date. We're going to discard the other columns. We can do that by doing df is equal to df and just the name of those two columns, which is date and then close. We'll set that and then outputting df, we should see only those two different columns.
We currently have a problem with our date column as it's actually not a date, it's just a string that has the year then the month then the day. We can see this if we type df sub date, we should see 'name of date' except it's a d type of object. We want that to be a date, we usually use this thing called date time. So we will import date time and then make a function. So we will define a function called 'string_to_datetime' which is going to take a string 's' which will be any of these strings here, any string that looks like this. We're going to pass that to the function in 's' and it's going to return the associated date time for that string. So in this function we'll create a variable called 'split' and set that equal to the 's.split' the hyphen which is the separator for each of these. So a 'split' is going to be the list of the year and then the month and then the day. We can extract those three pieces, 'year', 'month', and 'day' equal to the 'split' of zero, 'split' of one, and 'split' of two. These objects are actually strings right now, we want to make them integers so we'll just wrap each of them in 'int'.
so we can just return the datetime.datetime and then pass the year equal to our year the month equal to our month and the day equal to our day. We'll now test out our function by creating an object called date time underscore object equal to the string to date time. So, calling our function and we'll pass the first day in our data set which happens to be 1986-03-19. If we output this date time object we should see that it outputs datetime.datetime of 1986 319 and this is for the time but we don't need any of that now. What we need to do is apply this function to everything in our date column because we have in df this whole date column we want to make all of these date strings actual date objects. So, we'll set df subdate equal to itself so df subdate dot apply. So, we're applying a function to that column just the one that we made above. We can pass string to date time into this function. Note that we're not calling the function here we're passing the function itself to the supply function. Now, if we were to output our data frame column again df sub date should now show us the following. It looks like this is an error but this is just a warning this is okay. It looks like our column is the same as it was before but actually the d type is now date time 64. This is just what pandas converts it to. This is actually what we want. Our data frame is almost ready we just have one more step. If you look at df you can see that it's index this column right here is actually integers. We want to replace that column and make that date column the index. We can do that very easily by setting df.index equal to the df.pop which means we take away the column and return it. So df.pop passing the date and then outputting df we'll see that it did exactly what we desired make that date column the index and then we just have the closing value as a column. Now that we did that we can quickly plot our data using matplotlib. If we do import matpotlib.pyplot as plt we can do a plt.plot of the df.index and the df sub close. What we can see is from 1986 all the way up until 2022 the stock goes absolutely crazy after it hits about 2016 or so and then there's a little bit of a drop at the end. Now, because we're using the lstm model we need to convert this into a supervised learning problem. We're going to do this by making a new function. We'll call it define df to windowed df. It's going to take the data frame which will just pass df. It'll take a first date string and a last date string and then a positive integer we'll set it equal to 3 by default. This function is also going to need numpy so we're going to import numpy as np. Now it turns out that this code is extremely difficult to write.
And again, we want all the rows, so we'll put a colon. But we only want to start at the first column because we don't want that date column. And then we want to go up until, but not include, the last one. So, 1: -1 says all of these rows here, that's what the colon does. And then 1: -1 says just this piece of information in the middle. So, all of this piece. Now, unfortunately, what you'll find if you go through like that is that it's actually the wrong shape for the LSTM. We need x equal to middle matrix, but then we need to do a reshape. So, we'll do a reshape where the first dimension is the length of dates. So, this is the number of observations. That's pretty common for any tensorflow model. But now we need the second piece of this shape to be middle_matrix.shape[1]. That's just however many columns we had, and it would be the same as that n our window value. I'm just making it this because we have access to that. The last piece just has to be a 1 here because we are technically only using one variable. We have three different values of that variable and how it changes over time, but we're still only doing what we call univariate forecasting because we're just looking at how the closing value changes over time. If instead we had used some of those values at the very beginning, like the open, the high, the volume, and those variables, well then we'd have to put a different number down here. We'd have to put 2, 3, or 4 as this number. We're just doing 1 because we're doing univariate forecasting. Now, luckily, from here this function is very easy. We can just get our output vector y equal to df.as_matrix(), where again we want all of the rows but we only want the last column. That we can just do return dates, x, y. There's just a minor difficulty if you go on later. You'll see that has an error that we can fix with .astype(np.float32) actually np.float32 if we change those for x, and you also do that for y, y.astype(np.float32), you'll fix a weird error you'll find later. Now, to call this function we again want those three things. We'll get dates, x, y, and set that equal to windowed_df_to_date_x_y, just our function there. And we'll pass in our window_df from before. These three things should be numpy arrays, so we will get dates.shape, x.shape, and y.shape, and see that we have 9079 of each of these three things, our input matrix, and then 3x1, because we're looking three steps in the past, but for only one type of variable. Now, we're going to split the data into train, validation, and testing partitions. The training will train the model, the validation will help train the model, and then the testing is what we're going to use to really evaluate the performance of the model. We need two integers to help with the split. We'll get q80 first, that's going to be the integer of the length of dates times 0.8.
Then we'll get q90, which is equal to the int of the length of dates times 0.9. So we'll make the training partition the first 80 percent. So we'll get dates train, x train, and y train. Each of those are going to be each of their pieces. So this will be dates, but then only up until q80 to make it the first 80 percent. We'll do the same thing with x, so x up until q80, and then y up until q80. Because it's a little bit slow, I'm just going to paste in these two lines to get val, dates, val x, file and y about by going dates q80 to q90, then x q80 to q90, and y q80 to q90. That's all that information between the 80 and 90 pieces. Then we just get the testing information by saying q90 onward to get that last 10%. So you can see it's ordered: the first training piece is up until the first eighty percent, the validation is the eighty to ninety percent (10%), and then the test is that final ten percent from the ninety onward. We can visualize and color this very well with matplotlib, so we'll do plt.plot. Then we're going to get dates train, and then y train. We'll do the same for val, so dates underscore val and y val. Finally, the same for test, plt.plot dates test, and y test. And we'll just add in a legend, so that you can see which is which. Although it should be pretty obvious. Plt.legend train, then validation, then test. If you plot that, you're going to see that train is all this information here marked by the blue, then validation is this piece, and then test is this piece here. It's time to create and train our model. We can do a few imports from tensorflow: from tensorflow.comstep models get sequential. We're going to build a sequential model from tensorflow.curious.optimizers, we'll get atom. That's the optimizer we're going to use. And then from tensorflow.kira's import layers. We'll make a model that is sequential and built up of many layers. So we'll define our model and we're going to call it model is equal to a sequential, and then we'll pass that a list of layers. So the first one is just the input layers.dot.input, and we need to specify the shape of the input. Remember, we don't need to specify the batch number or how many examples. Three by one, again it's three because we're doing three days in the past, and that's one because we need only one feature, only univariate forecasting. Now that we've specified the input layer, we're ready to do an lstm layer. So we will do layers.dot.lstm and capitals, and this number is relatively arbitrary.
But we will choose 64, which is a relatively big (but not super big) number of neurons for the LSTM. All that you really need to know about this number is the bigger the number, the more complicated the model is, the more prone it is to overfitting, and the more heavy-duty it is considered. We will add instead of an LSTM, a dense layer (layers.dense). We will choose 32 for a similar reason as above. You're also very welcome to stack dense layers, and so we'll just actually paste that in again and have another 32.
We're not going to mess with the activation functions for the LSTM, but for the dense layers, it's usually a good idea to set activation equal to relu, so we will do that for both of those dense layers. Now we must specify the output of our model, and since we are only forecasting one variable (we're just trying to predict say the next value), we only want this to be a layers.dense of one, where we don't change the activation function, as by default it's linear, which is desired.
We can now close this up and specify that the model is going to be compiled. To compile the model, we must set the loss function, and the loss function we want to minimize is the mean squared error (MSE), so we will just write the string of mse for mean squared error. We also need to specify the optimizer, so we will set the optimizer equal to the Adam optimizer, where we specify that the learning rate is equal to 0.001. It turns out that 0.001 is going to work out pretty well for this example, but if you're doing a different problem, the learning rate is something you definitely want to play around with, as well as these values here.
We also want to specify a new metric, which is going to be metrics equals, we need to put it in a list, the mean absolute error (MAE). This number tells us on average how much we're off by, rather than the squared distance. We'd rather look at this, although we need to minimize the MSE as this is not differentiable.
We're now ready to fit the model, so we can do model.fit. We pass our inputs of x_train and y_train, and then we specify that the validation data is equal to the tuple of x_val and y_val. We're going to let this run for 100 epochs, which means 100 runs through the data set. I'm going to press enter and we can see what happens. As we can see at this point, it looks like it's not really changed very much, so we can actually cancel this, and it is going to save whatever progress it's done so far.
Now, to briefly analyze this, we mostly care about the validation mean absolute error going down. We can see it's at 14 at the beginning, then it goes to 11, 10, 9.
And then it hovers around 8, 9, 10, and that's when I was ready to stop it because it wasn't really changing all that much. It's much easier to visualize what's going on instead with graphs, so before worrying about the code, I'm just going to show you the pretty picture we can make for it predicting on the training set. So the orange is the actual observed observations; it's what really happened from 1986 to 2016. The blue is what we predicted. So each time, it got the three previous, and it tried to predict the next one. That's also what it was trained on. To make that run, we simply get the training predictions with model.predict on x_train, and then we have to do a flatten. Then we can do a plot of dates_train and the train_predictions and dates_train and y_train. That's that blue and the orange curve, and then we just create the legend. Since I explained that code for the train, I feel no real need to explain it much for the validation, as this is literally the same thing, but replacing the word "train" with "val". So for the validation, we get this graph. Or, it follows it until you know about 2017, and then it just really flattens off, which is the same time when it actually starts to pick up. So the observations, what really happened, is it went up like this, but the predictions, it actually just started to zone off and it couldn't follow it anymore. If we were to look at the test, this is again just replacing that word "train" with "test". This picture is even worse; it doesn't follow it at all. It actually thinks it's going down a little bit, whereas it's going up a lot, and then it goes down. I'm now going to put all three of those pictures on the same graph. Again, the code is not hard; it's just annoying. Where we first plot the training predictions and the observations, the validation predictions and the observations, same for the test, and then we create the legend. We see that this picture, again for the training, it follows it very closely. And for the red piece, is what actually happened in validation. The green is what it thought happened. Not good at all. The brown is what really happened, and the purple is what it thought for the test. Really, really bad at that point. It turns out that these LSTM models are very bad at what we call extrapolating, and so if it was trained on data only in this range here, only up until like the 50 value, it's not going to be good at predicting stuff this high, even though it is given, say, his input, these three values here, and has no idea what to do with them, because it's not going to extrapolate well. Extrapolate means basically learn data outside its range. A line extrapolates well, because if we drew a line here, we could just continue drawing that line up like that. But if the LSTM is only trained on this data here, it will have no idea what to do when the values are increasing and are this big. Another way to think about it is that all this information here, it might actually not be that helpful, because over here the values are way up like this, and the pattern starts changing a lot.
So, maybe we don't want to train it on all of this. Maybe we just want to train it on say this information here, and then validate over here. So, we'll do just that. We're going to pick some day over here to start training at. We do need to know that this date is actually in the data set, and for that we'll go to our data set over here and select the time period of one year. And if we apply that, we just need to scroll back all the way to the bottom and see that one date that we know exists is March 25th, 2021. We will use that as our starting value instead. So, that means we need to change our windowed function above, or not actually change the windowed function itself, but just change how we're calling it. We need to change this value here to be the year is going to be 2021, 03 is fine, and then 2 5 is a date we know exists as you can see here. I had this in a comment for me to remember. So, now the first date will be 2021 0325 and these are its corresponding information. The end date is exactly the same, and we only have 252 rows this time, way less information. We should have no problem just re-running the cells we already did. So, we're going to do that, which gets dates x and y. Note that they're smaller this time. We'll again split the data set and make sure that we plot it properly. So, our starting date up until about the middle over here is train, then validation, then test. And note that we've already seen values in this range, so it should be okay to predict values in the same range over here since we only change the number of things the model is seeing. The model is actually fine as is. We can run that again and it's going to run a lot faster now. We'll see again that our mean absolute error goes down pretty low and for the validation a lot better than it was before. We can recreate all of our graphs. So, to plot the training, we can see here the train it doesn't follow it quite as well as before, but that's totally okay. If we see here for the validation, it got so much better. Now look at how zoomed in this is. These values are extremely close to each other, and if we were to do it for the test as well, the tests are also extremely close to each other. If we were to plot them all on the same graph again, we would see here zoomed out that they're all very close to each other. The predicted first, the observation is very, very close, no matter whether it's the train, the validation, or the test. Now the video could be done here, but I want to show you how you could try and predict long term, because all of these values, any of these predictions we're assuming we had the actual three days before and that data was real, then we used those three days before to make the prediction and then the next day we would have the actual three.
And then, we'd use that to predict the next day. Well, what we're actually going to do is train here and then pretend that's all the data that we have and let the model recursively predict the future and see what it has to say. So, to make that function, we're first going to do from copy import deepcopy. We'll make a list and start to build this up called recursive_predictions is equal to an empty list. And then, we'll get recursive_dates. These are the dates on which we're predicting. For this, it's already known and this is equal to np.concatenate(dates_val, test_val). This is because the dates we're predicting are here onward, so we're training on all of this. In fact, we've already trained on all of that and then the recursive_predictions are going to be for these following dates. So now, we can loop through those dates: for target_date in recursive_dates: We'll get our most recent input, so the last, we'll call it window. I'm just copying it so we don't change anything, deepcopy(x_train[-1]) because the last window that we actually had access to was the very last three over here that is stored in x_train[-1]. And we need to start predicting for the future, so we need to get our next prediction. That will be equal to model.predict(np.array([window])).flatten(). Unfortunately, we actually have to make it the numpy.array of the list of the last window, but really it's just the last window. Don't worry too much about that piece. And then flatten it like before. Then, what we can do is recursive_predictions.append(next_prediction). So, add that to our list with next_prediction. Then, we need to update this last window because we just made a prediction for the next day. Well, now we need to move on to the previous two informations that were actually seen and then the next predicted value, because we need to start using the values that we're predicting. That's why it's called recursive predicting. So, we'll actually set last_window[-1] = next_prediction.
Sorry, I have an error here. This should actually be dates test. And then if we run that, I'm now going to paste in again some annoying code, but it'll look very familiar. It's exactly the same as that big graph as before, except I added in the recursive dates and the recursive predictions, and that put that in the legend as well. If I were to plot this, you will see something very funny. This piece right here is the recursive predictions. The model has absolutely no idea on how to predict in the future. It just thinks it'll be what it was before, and actually that's a reasonable prediction. Predicting stocks is incredibly difficult. There is of course the trend we can analyze. We saw before that the graph really started to go up, and that would indicate to you that it's a good stock to buy. But that doesn't mean I can guarantee that, and I don't want to be liable for you predicting any sort of stocks with any sort of model. And by no means is the model we made useless. It's just on the micro scale of per day, should I sell or buy. Of course, in general, people generally think of stocks for the long term, what should I do to make money in the long term. But on a micro scale, it's important to know as well. So I hope you enjoyed that video. If it brought you value, please drop a like and consider subscribing. It really really helps, and I'll see you next time, guys.